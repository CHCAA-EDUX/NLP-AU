{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Class 6: Word embeddings\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation for class\n",
    "---\n",
    "\n",
    "So was sick friday so didn't get the chance to get this out before the weekend. So you really don't have to prepare anything, but I might be worth going through the gensim section code to get an overview before class and to make sure that everything work as intended."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gensim\n",
    "---\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can install gensim simply using: \n",
    "```py\n",
    "pip install gensim\n",
    "```\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using Gensim\n",
    "to start us of we can download the word embeddings we will use for class using:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Pre-trained vectors based on Wikipedia 2014 + Gigaword, \n",
    "# 5.6B tokens, 400K vocab, \n",
    "# uncased (https://nlp.stanford.edu/projects/glove/).\n",
    "# embedding size 50 (so fairly small, but easy to work with)\n",
    "word_emb = api.load(\"glove-wiki-gigaword-50\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can the parts of the word embedding class here, but I do encourage you to inspect it a bit more using e.g. `dir(word_emb)` or `help(word_emb)`:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "print(type(word_emb))\n",
    "\n",
    "# inspecting the vocabulary\n",
    "vocab = word_emb.vocab.keys()\n",
    "vocab = sorted(list(vocab)) # sort the vocab\n",
    "print(vocab[:10])\n",
    "print(vocab[51000: 51010])\n",
    "print(vocab[-10:])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n",
      "['!', '!!', '!!!', '!!!!', '!!!!!', '!?', '!?!', '\"', '#', '##']\n",
      "['alemtuzumab', 'alemu', 'alemán', 'alemão', 'alen', 'alena', 'alencar', 'alencon', 'alendronate', 'alene']\n",
      "['門下省', '魏博', '鳳翔', '법정동', '행정동', 'ﬁeld', 'ﬁgures', 'ﬁnds', 'ﬁrst', '￥']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now extract word embeddings for words in the vocabulary:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# fetch the word embedding for class\n",
    "glass = word_emb[\"glass\"]\n",
    "print(type(glass))\n",
    "print(glass.shape) # fairly small word embedding\n",
    "\n",
    "print(\"aarhus\" in word_emb) # check if word in vocabulary\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50,)\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you probably know from the lectures the similarity between word embedding is calculated using cosine similarity, which you might recall we implemented in the introduction week. We can use this to calculate the similarity between a couple of vectors here:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\n",
    "def cosine(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "for w1, w2 in [(\"dog\", \"dog\"), (\"dog\", \"glass\"), (\"dog\", \"cat\")]:\n",
    "    sim = cosine(word_emb[w1], word_emb[w2])\n",
    "    print(f\"the similarity between {w1} and {w2} is {round(sim, 2)}\")\n",
    "\n",
    "# You can also calculate this using gensim:\n",
    "word_emb.similarity('dog', 'glass')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the similarity between dog and dog is 1.0\n",
      "the similarity between dog and glass is 0.27000001072883606\n",
      "the similarity between dog and cat is 0.9200000166893005\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.27108672"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You could imagine doing this for all words in the corpus to get the most similar words to `dog`, using gensim this can be done simply using:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(word_emb.most_similar(\"dog\", topn=10))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('cat', 0.9218005537986755), ('dogs', 0.8513159155845642), ('horse', 0.7907583713531494), ('puppy', 0.7754921913146973), ('pet', 0.7724707722663879), ('rabbit', 0.7720813751220703), ('pig', 0.7490061521530151), ('snake', 0.7399188280105591), ('baby', 0.7395570874214172), ('bite', 0.7387937903404236)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function also helps you do analogies, *\"woman is to ____ what man is to king\"* or rephrased *king-man + woman = ?*:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# you can also do analogies using this function:\n",
    "sim = word_emb.most_similar(positive = [\"woman\", \"king\"], negative=[\"man\"], topn=1)\n",
    "print(sim)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('queen', 0.8523603677749634)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naturally the phrase *\"king - man + woman = ?\"* is nonsense, you can't do mathematics with words, but you can with word embeddings. So let us just go through what is going on here:\n",
    "We start of taking the word embedding of king $w_{king}$ substract the word embedding for man $w_{man}$ and then add the word embedding for women $w_{woman}$, thus we have: \n",
    "\n",
    "$$w_{anology} = w_{king} - w_{man} + w_{woman}$$\n",
    "\n",
    "Naturally it would be extremely coincidental that we were to have $w_{anology}$ being equal to $w_{queen}$, what we actually do is calculate the similarity between all vectors and $w_{anology}$ and find that $w_{queen}$ is the closest to that one, thus: \n",
    "\n",
    "$$w_{anology} \\sim w_{queen}$$\n",
    "\n",
    "Do note that we are working with vectors and that these are [cummutative](https://en.wikipedia.org/wiki/Commutative_property) (e.g. $v_1 + v_2 = v_2 + v_1$) thus it is just as valid to write:\n",
    "\n",
    "$$w_{anology} = w_{king} + w_{woman} - w_{man} = w_{king} + (w_{woman} - w_{man})$$\n",
    "\n",
    "Where you can interpret $w_{woman} - w_{man}$ as the *gender component* assuming that everything else about the usage of man and woman is similar i.e. they both refer to human, typically adult and seen as a neutral gendered description of a person. Even in simple cases such as this one it is trivially not true, *man* can even refer to a member of mankind (*god cares for all men*) an authority (*the man*) or can be used as an exclamation (*oh man*). \n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary> That sounds like a problem, what do we do to solve it? </summary>\n",
    "\n",
    "Glad you ask. Well there are multiple and probably more than I don't know of. One way is to concieve the other meanings as noise in the *gender component* and one way to remove it is to average over mulitple gender components (another one could be $w_{girl} - w_{boy}$), you could probably imagine others as well. Another way is to disambiguate the word embeddings (by created the word embedding for each different meaning of the word), not a trivial task, but one place where you can start is including the part-of-speech tag and thus distingushing between e.g. *desert* as verb or *desert* as a noun. Word sense disambiguation is still an active area of reasearch.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<br /> \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting word embeddings\n",
    "We naturally can't plot a word embedding with an embedding size larger than three so what we can do instead is reduce the dimensionality e.g. using a PCA, which some of you might be familiar with.\n",
    "\n",
    "<details>\n",
    "    <summary> What the hell is a PCA? </summary>\n",
    "\n",
    "It is apparently a part of experimental methods II, but I must admit I don't really recall hearing much about it either. PCA stands for principal component analysis and reduces a matrix (a bunch of datapoints) set of principal components which are features which seek to explain the most variance in the data while remaining uncorrelated. For instance imagine if you had height, gender, education, parents education as variables we know that there is a lot of redudancy of information (gender correlated with height and education with parents education), thus you could imagine reducing that to two components a *'gender'*-component and a *'social status'*-component. This is what PCA tries to do. \n",
    "If you want to know more there is a [20 minutes video](https://www.youtube.com/watch?v=FgakZw6K1QQ&t=2s) or a [5 minute video](https://www.youtube.com/watch?v=HMOI_lkzW08) both are by Josh Starmer, which some might find frustratingly pedagogic while other might (like me) find him hilarious. \n",
    "\n",
    "</details>\n",
    "\n",
    "<br /> \n",
    "\n",
    "<details>\n",
    "    <summary> Whouldn't it be better to use a another dimensionality reduction approach? </summary>\n",
    "\n",
    "Probably, PCA isn't really intended for plotting. T-SNE is a good alternative as it preserves local structure (but disregards global structure). However PCA is quick as constitutes a very solid baseline and as it is only for inspection I don't believe you loose much.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br /> \n",
    "\n",
    "I have made a function for it here which you are free to use, but do take a moment to read through it.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\n",
    "def plot_word_embeddings(words, embedding):\n",
    "    \"\"\"Plot the words embedding in 2D using a scatter plot and PCA\n",
    "\n",
    "    Args:\n",
    "        words (List[str]): List of words\n",
    "        embedding: Your word embedding as  gensim keyed vectors object.\n",
    "\n",
    "    Returns:\n",
    "        A matplotlib plot object\n",
    "    \"\"\"\n",
    "    # extract word embedding matrix\n",
    "    X = embedding[words]\n",
    "\n",
    "    # apply a dimensionality reduction method of your choosing e.g. PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(X_pca, index=words, columns=['x', 'y']) # create a dataframe for plotting\n",
    "\n",
    "    # create a plot object\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    # add point for the words\n",
    "    ax.scatter(df['x'], df['y'])\n",
    "\n",
    "    # add word label to each point\n",
    "    for word, pos in df.iterrows():\n",
    "            ax.annotate(word, pos)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "words_to_plot = [\"man\", \"woman\", \"queen\", \"king\", \"boy\", \"girl\", \"actor\", \"actress\", \"male\", \"female\"]\n",
    "ax = plot_word_embeddings(words=words_to_plot, embedding=word_emb)\n",
    "ax.plot()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Plan for class\n",
    "---\n",
    "\n",
    "- We will go through the `super().__init__()` to get an initial understanding.\n",
    "- Work on exercises\n",
    "- Last 15 minutes: present solutions\n",
    "\n",
    "<!-- Some exercises\n",
    "- most similar \n",
    "- king - man + woman = queen\n",
    "- NER\n",
    "\n",
    "NER (anonymization)\n",
    "\n",
    "- download a word embedding\n",
    "\n",
    "\n",
    "- visualizing word embeddings (PCA)\n",
    "\n",
    "\n",
    "find the most similar word embeddings\n",
    "\n",
    "plan for class:\n",
    "    - We will go through the `super().__init__()` to get an initial understanding.\n",
    "    - Classify named entities -->\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Understanding Super class initialization\n",
    "We will go through this example at the start of class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class person():\n",
    "    def __init__(self, name, year_of_birth):\n",
    "        self.name = name\n",
    "        self.age = 2021-year_of_birth\n",
    "    \n",
    "    def say_hello(self):\n",
    "        print(f\"Hello my name is {self.name} and I am {self.age} years old\")\n",
    "\n",
    "class researcher(person):\n",
    "    def __init__(self, loc, employment):\n",
    "        self.loc = loc\n",
    "        self.employment = employment\n",
    "        \n",
    "    def say_hello(self):\n",
    "        print(f\"Hello I am currently a {self.employment}, at {self.loc}\")\n",
    "\n",
    "\n",
    "kenneth = researcher(\"Aarhus\", \"phd\")\n",
    "kenneth.say_hello()\n",
    "# problem 1:\n",
    "    # I want to reuse code from the init of person without copy pasting\n",
    "# problem 2:\n",
    "    # I want to be able to call the say_hello function of the parent class\n",
    "    # e.g. kenneth.say_ordinary_hello()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello I am currently a phd, at Aarhus\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the pytorch nn.Module init then do? Well from the [source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module) we find the following:\n",
    "\n",
    "```py\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
    "        \"\"\"\n",
    "        torch._C._log_api_usage_once(\"python.nn_module\")\n",
    "\n",
    "        self.training = True\n",
    "        self._parameters = OrderedDict()\n",
    "        self._buffers = OrderedDict()\n",
    "        self._non_persistent_buffers_set = set()\n",
    "        self._backward_hooks = OrderedDict()\n",
    "        self._is_full_backward_hook = None\n",
    "        self._forward_hooks = OrderedDict()\n",
    "        self._forward_pre_hooks = OrderedDict()\n",
    "        self._state_dict_hooks = OrderedDict()\n",
    "        self._load_state_dict_pre_hooks = OrderedDict()\n",
    "        self._modules = OrderedDict()\n",
    "```\n",
    "\n",
    "Which notably sets a couple of internal states (noticably self.training=True), and log it to the pytorch API."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercises\n",
    "---\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "you are not required to solve these tasks in any particular order. Exercise 6 is more challenging than the rest so if you are up for a challenge go for it.\n",
    "\n",
    "1) Skim/read through the Gensim section to get an overview of how to work with word embeddings. \n",
    "2) What is to woman what man to doctor (according to the word embedding)? Is this problematic?\n",
    "3) Discuss how you could use word embeddings to find pluralis of a word\n",
    "  - apply it to three cases\n",
    "  - plot these, does there seem to be a *plural component*?\n",
    "3) Discuss how you would find the antonym of a word\n",
    "  - apply it to three cases\n",
    "  - plot these, does there seem to be a *antonym component*?\n",
    "4) You can also use word embedding to detect the odd one out for instance which word does not belong in *Ross, Kenneth, Tim, glass*? You can do this simply using: `word_emb.doesnt_match(\"kenneth ross tim glass\".split())`. Try this out on three samples.\n",
    "  - Typically word embeddings are quite intuitive, discuss or write down your best answer to how one might find the odd one out?\n",
    "\n",
    "<details>\n",
    "    <summary> Answer </summary>\n",
    "\n",
    "The function works by:\n",
    "\n",
    "- taking the mean of all the word-embeddings.\n",
    "- calculate the similarity (typically using cosine-similarity) from the mean to each word\n",
    "- return the most dissimilar word (i.e. the one with the highest cosine-distance from that mean vector).\n",
    "\n",
    "Do note that while your answer might not be the same it might still work as there are multiple ways of doing this. For instance you could also add up the distance between each word and every other word and see which one is the furthest away from all other points.\n",
    "\n",
    "</details>\n",
    "\n",
    "5) Which word embedding is closest to the word embedding of *corona*; *virus* or *beer*? Justify your answer before you test it using the word embedding.\n",
    "  - What does this tell you about word embeddings?\n",
    "6) Word embedding for token classification\n",
    "  - We will here use token embeddding for classifying part-of-speech tags (i.e. whether a word is a noun or a verb), but this approach could just as well be used to classify whether a word is named entity or whether it is positive or negative. We will here use the English dependency treebank. In the utilities section below I have provided some functions to get you started, but I recommend solving it in 3 steps:\n",
    "    1) transform the words to word embeddings\n",
    "    2) Train a classifier to predict the pos-tag using e.g. logistic regression\n",
    "    3) calculate the performance metrics on the held out test set (how well does it perform on unseen words?)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilities for tagging POS using word embeddings\n",
    "This contain a bunch of utility functions for getting started using word embeddings for classifying part of speech tags.\n",
    "\n",
    "Including a 1) logistic classifier (or a softmax classifier), 2) a function to load in the dataset as a list of token, pos-tag pairs, and 3) a code snippet for turning the label into one hot vectors.  \n",
    "\n",
    "You can read more about the individual [pos-tags](https://universaldependencies.org/docsv1/en/pos/all.html) here."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\n",
    "\n",
    "from spacy.training import Corpus\n",
    "import spacy\n",
    "\n",
    "# change this so it fits your file structure:\n",
    "path = os.path.join(\"..\", \"data\", \"English Dependency Treeback\", \"en_ewt-ud-train.spacy\")\n",
    "# and use en_ewt-ud-dev.spacy and en_ewt-ud-test.spacy for the dev and test set.\n",
    "\n",
    "def load_pos_data(path: str) -> list:\n",
    "    corpus = Corpus(path)\n",
    "\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    def extract_token_label_pair(example):\n",
    "        e = example.to_dict()\n",
    "        tokens = e[\"token_annotation\"][\"ORTH\"]\n",
    "        entity_labels = e[\"token_annotation\"][\"POS\"]\n",
    "        return zip(tokens, entity_labels)\n",
    "    train_samples = [(token, label) for example in corpus(nlp) for token, label in extract_token_label_pair(example)]\n",
    "\n",
    "    return train_samples\n",
    "\n",
    "train = load_pos_data(path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# exploring the output\n",
    "print(train[:10])\n",
    "\n",
    "tokens, labels = list(zip(*train))\n",
    "print(tokens[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('Al', 'PROPN'), ('-', 'PUNCT'), ('Zaman', 'PROPN'), (':', 'PUNCT'), ('American', 'ADJ'), ('forces', 'NOUN'), ('killed', 'VERB'), ('Shaikh', 'PROPN'), ('Abdullah', 'PROPN'), ('al', 'PROPN')]\n",
      "('Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# convert labels to one hot\n",
    "import pandas as pd\n",
    "df = pd.get_dummies(labels)\n",
    "y = df.to_numpy() # One hot representation of the labels\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>AUX</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PART</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>SYM</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ADJ  ADP  ADV  AUX  CCONJ  DET  INTJ  NOUN  NUM  PART  PRON  PROPN  PUNCT  \\\n",
       "0    0    0    0    0      0    0     0     0    0     0     0      1      0   \n",
       "1    0    0    0    0      0    0     0     0    0     0     0      0      1   \n",
       "2    0    0    0    0      0    0     0     0    0     0     0      1      0   \n",
       "3    0    0    0    0      0    0     0     0    0     0     0      0      1   \n",
       "4    1    0    0    0      0    0     0     0    0     0     0      0      0   \n",
       "\n",
       "   SCONJ  SYM  VERB  X  \n",
       "0      0    0     0  0  \n",
       "1      0    0     0  0  \n",
       "2      0    0     0  0  \n",
       "3      0    0     0  0  \n",
       "4      0    0     0  0  "
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Logistic(nn.Module):\n",
    "    def __init__(self, n_input_features: int, n_output_features: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(n_input_features, n_output_features)\n",
    "        self.cost = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x)\n",
    "    \n",
    "    def fit(self, X: torch.Tensor, y: torch.Tensor, epochs = 1000):\n",
    "        optimizer = torch.optim.AdamW(self.parameters())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # forward\n",
    "            y_hat = self.forward(X)\n",
    "\n",
    "            # backward\n",
    "            loss = self.cost(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"epoch: {epoch+1}, loss = {loss.item():.4f}\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## More on Word embeddings\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Overview and introduction of word embeddings:\n",
    "\n",
    "- Andrew NG have a small [video series](https://www.youtube.com/playlist?list=PLhWB2ZsrULv-wEM8JDKA1zk8_2Lc88I-s) on word embedding, which I highly recommend watching\n",
    "\n",
    "More by Mikolov et al.:\n",
    "\n",
    "- For the readings for class read on article by Mikolov et al. (2013) however the same year the team released an additional [paper](https://arxiv.org/abs/1310.4546) called *Distributed Representations of Words and Phrases and their Compositionality*. There is even a [video reading](https://www.youtube.com/watch?v=yexR53My2O4) of the paper which takes you through the main points.\n",
    "\n",
    "Cross-lingual word embeddings:\n",
    "\n",
    "- Wish I had the time to go through Cross-lingual word embeddings. The idea is that you overlay word embeddings for two languages and then this act as a way of translating between languages. It is a really cool application of word embeddings so if you feel like digging into it I recommend [this podcast episode](https://soundcloud.com/nlp-highlights/57-a-survey-of-cross-lingual-word-embedding-models-with-sebastian-ruder) (in general this podcast is great). You can also watch Kevin Clark (Manning's PhD student) give a [talk](https://www.youtube.com/watch?v=3wWZBGN-iX8&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=20) on how use word embeddings for translation without any training data.\n",
    "\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit ('NLP': virtualenv)"
  },
  "interpreter": {
   "hash": "2136a9c3637fd160483224d7922e48bf03b650be5dff26724a0c1f8d1279953b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}