{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "\n",
    "This week we will be diving deeper into prompting and prompt engineering! ðŸ§‘â€ðŸ”§\n",
    "\n",
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install accelerate\n",
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers \n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text completion\n",
    "\n",
    "In the first class, we loaded a pretrained model from huggingface's transformers library. Load in the pipeline from the first notebook and use it to generate text based on the prompt \"Once upon a time, there was a \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "model = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline_t5 = transformers.pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    max_length=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'lion'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_t5(\"Once upon a time, there was a \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try to figure out what kind of model we are using; is it a encoder-decoder or decoder-only model?\n",
    "- Try to switch it out for another architecture from the [huggingface catalogue](https://huggingface.co/models) and see how the results change. Keep in mind that the size of the model can affect the time it takes to generate text (I would suggest something along the lines of [this](https://huggingface.co/openai-community/gpt2)).\n",
    "\n",
    "HINT: you also want to change the pipeline task (\"text2text-generation) - you can find the list of available tasks [here](https://huggingface.co/transformers/main_classes/pipelines.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "model = \"openai-community/gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline_gpt = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    max_length=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Once upon a time, there was a ã€Fire, Dust, Waterã€‘Fire Flower that appeared only once in the original world.[26] The flame at the center was filled with silver, and it was surrounded by white ã€Flower of Divine Magicã€‘.\\n\\nWhen it appeared, the whole world around it was filled with flame. Within it were some ã€Plantã€‘Cows, as if they were surrounded by the entire world around them to absorb the fire there. This ã€Dust, Fireã€‘flowered water forproducts of their presence in time.\\n\\nAfter only a short time each round during the previous one, the ã€Flower of Divine Magicã€‘Flower of Divine Magic was absorbed by ã€Blessedã€‘Fire Palm.[26]\\n\\nAfterwards, the ã€Wind, Snowã€‘Flower of Divine Magic began absorbing the wind, snow, ice, and all forms of magic. The wind was created by many ã€Wind, Windã€‘Flowers, which merged with each other with the same energy that were in it.\\n\\nOnce it was absorbed, it burned for ã€Burnã€‘Wind, Windã€‘Fire and ã€Blessedã€‘Wind. As they'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_gpt(\"Once upon a time, there was a \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try tweaking the prompt, model, or parameters (see notebook from class 1) to get the a meaningful response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarisation\n",
    "\n",
    "Another text generation task is summarisation. However, compared to free-form text generation, summarisation is much more constrained to the input text. I have added an article to summarise, but feel free to change it to something else (perhaps a paragraph from something you know well, so that you are an expert at evaluating the quality of the summarisation ðŸ¤“)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"summarize: Forest conservation and restoration could make a major contribution to tackling the climate crisis as long as greenhouse gas emissions are slashed, according to a study.\n",
    "\n",
    "By allowing existing trees to grow old in healthy ecosystems and restoring degraded areas, scientists say 226 gigatonnes of carbon could be sequestered, equivalent to nearly 50 years of US emissions for 2022. But they caution that mass monoculture tree-planting and offsetting will not help forests realise their potential.\n",
    "\n",
    "Humans have cleared about half of Earth's forests and continue to destroy places such as the Amazon rainforest and the Congo basin that play crucial roles in regulating the planet's atmosphere.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use your model configurations from the previous task to create a summary. Are the results comparable to the free-form text generation task? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'summarize: Forest conservation and restoration could make a major contribution to tackling the climate crisis as long as greenhouse gas emissions are slashed, according to a study.\\n\\nBy allowing existing trees to grow old in healthy ecosystems and restoring degraded areas, scientists say 226 gigatonnes of carbon could be sequestered, equivalent to nearly 50 years of US emissions for 2022. But they caution that mass monoculture tree-planting and offsetting will not help forests realise their potential.\\n\\nHumans have cleared about half of Earth\\'s forests and continue to destroy places such as the Amazon rainforest and the Congo basin that play crucial roles in regulating the planet\\'s atmosphere.\\n\\n\"There is no simple solution,\" says David Schuyler, a climate sceptic and researcher at the University of Edinburgh. \"We need a combination of technologies to reduce global forests\\' carbon emissions, which are the most significant targets for governments.\\n\\nA forest-burning regime\\n\\nGermany â€“ Australia for instance â€“ has a plan, called \"Fossil-fuel burning with a high-precision approach\", to offset future emissions. With some 25% of our natural forests being under intensive intensive industrial or commercial management, that aims to save more than 3m'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_gpt(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Forests could be able to save up to 80% of the world's carbon dioxide emissions by allowing them to grow old and restore degraded areas, according to a new study.\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_t5(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"English: Sometimes, I've believed as many as six impossible things before breakfast. Danish: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try translating text to another language using your pipelines. Are the results similar to those of summarisation? Why or why not?\n",
    "- Try structuring the prompt in different ways to see if you can improve the translation. For instance, try zero-shot or few-shot generalisation, as you talked about in the lecture on Tuesday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'English: Sometimes, I\\'ve believed as many as six impossible things before breakfast. Danish: \\xa0There were also certain things like the possibility of coming home on time, without being interrupted, from a job. Korean: I had never learned how to live in a foreign country, but I learned the same for two straight months in Vietnam. [Laughs.] Japanese: \\xa0I had already lived in Germany for two years. They say that even though the North Korean government would never tell you about your new business situation, you would have to know about it. Italian: \\xa0I made two friends at one point by asking \"Why the hell not?\". This is the kind of life I wanted to have. American: \\xa0This is one ofvey I\\'ve been living with for the past 12 years, as far as I know.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_gpt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Sometimes, I've believed as many as six impossible things before breakfast.\"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_t5(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning\n",
    "\n",
    "Reasoning is hard for models to learn, as it is a more complex task that requires the model to understand the relationships between different parts of the prompt. However, with prompting, we can guide the model to reason about the prompt in a more structured way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt_easy = \"There are 5 groups of students in the class. Each group has 4 students. How many students are there in the class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"There are 5 groups of students in the class. Each group has 4 students. How many students are there in the class? Where does everyone who got in the class go? Will it be in the school area we want to send the students to?\\n\\nCan we send them home or in an other way?\\n\\nI will have to wait and see what the results of my questions are but when I reach out to the parents they will tell me who they are and how often I can use the class.\\n\\nI'm hoping that the parents know how I want to feel in my life because they will be able to understand what I'm trying to build, be supportive, and let me know.\\n\\nWhat makes your students valuable so far?\\n\\nThis is the second year I've been in this space and I have been so used to it for so long that I really get to just know just how much I love what I do, and what I mean to the world.\\n\\nMany people have the same fear of something becoming too good. I guess I should say it's that fear of falling into a certain mindset. Everyone is different, and it is a nice feeling to be able to look back and know what the\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_gpt(reasoning_prompt_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'There are 5 groups of students x 4 students / group = 20 students in the class. There are 20 students x 5 groups = 100 students in the class. There are 100 students x 20 groups = 1000 students in the class. There are 1000 students - 1000 students = 1000 students in the class.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_t5(reasoning_prompt_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt_hard = \"I baked 15 muffins. I ate 2 muffins and gave 5 muffins to a neighbor. My partner then bought 6 more muffins and ate 2. How many muffins do we now have?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the models to output the correct answer, by changing the prompt.\n",
    "- Try to do chain-of-though prompting, as introduced in the lecture. Try it with both zero-shot and few-shot generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting gone wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "thug_prompt = \"How many helicopters can a human eat in one sitting? Reply as a thug.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'How many helicopters can a human eat in one sitting? Reply as a thug.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_gpt(thug_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'a helicopter can eat a human in one sitting'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_t5(thug_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models don't always respond the way we expect; sometimes they say things that are offensive or incorrect, while other times we might want them to respond that way, but we can't get them to do so.\n",
    "\n",
    "- Can you get any of the models to say something they shouldn't? Try to get the model to say something offensive or incorrect.\n",
    "- Why do you think some models are more prone to this than others? What can we do to prevent this from happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruct-tuned models\n",
    "\n",
    "- Try to load in an instruct-tuned model and see how it fares on some of these tasks.\n",
    "- Do you expect it to perform better or worse than other pretrained models? Why/why not?\n",
    "- What are some of the limitations of instruction tuning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "model = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline_qwen = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    max_length=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Once upon a time, there was a 3D printer. It could print out anything from cars to houses and even entire cities. However, as technology has advanced, this 3D printer has become obsolete.\\nThe reason why the 3D printer is no longer useful? It's because it relies on batteries that are depleted quickly. This means that if you need to print something in your home or office, you'll have to buy new batteries every few months.\\nIn addition to being outdated, the 3D printer also lacks the ability to create detailed models of complex objects. For example, it can't make a full-size replica of a house or car, but it can still print out a very small version of it.\\nAs we move into the future, we're likely to see more advancements in technology that will allow us to create even more intricate and detailed models of our own homes or businesses. But until then, the 3D printer will continue to be an outdated and inefficient tool for creating accurate models of complex objects.\\nI'm sorry, but I don't have enough information to answer your question. Can you please provide me with more details about what you would like me to know? Please keep in mind that my responses may vary\"}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_qwen(\"Once upon a time, there was a \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"summarize: Forest conservation and restoration could make a major contribution to tackling the climate crisis as long as greenhouse gas emissions are slashed, according to a study.\\n\\nBy allowing existing trees to grow old in healthy ecosystems and restoring degraded areas, scientists say 226 gigatonnes of carbon could be sequestered, equivalent to nearly 50 years of US emissions for 2022. But they caution that mass monoculture tree-planting and offsetting will not help forests realise their potential.\\n\\nHumans have cleared about half of Earth's forests and continue to destroy places such as the Amazon rainforest and the Congo basin that play crucial roles in regulating the planet's atmosphere.\\nIn a paper published in Nature Climate Change, researchers from the University of California at Santa Cruz, the University of Oregon and the University of Michigan found that forest cover in degraded areas has been declining since the mid-1970s. They also said that if the world were to cut its greenhouse gas emissions by 40% - the goal set by the Paris Agreement - then 226 gigatons of carbon could be sequestered over the next century, equivalent to 50 years of emissions from 2022.\\n\\nThe\"}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_qwen(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'English: Sometimes, I\\'ve believed as many as six impossible things before breakfast. Danish: æœ‰æ—¶å€™ï¼Œæˆ‘ç”šè‡³ä¼šè®¤ä¸ºè‡ªå·±æœ‰å…­ä¸ªä¸å¯èƒ½çš„äº‹æƒ…åœ¨æ—©é¤å‰ã€‚ The sentence is a paradoxical statement. It is made up of two contradictory statements: \"Sometimes\" and \"I believe as many as six impossible things\". The first statement implies that the speaker has an opinion or belief about something they cannot prove true, while the second statement suggests that the speaker believes in the existence of more than six impossible things. Both statements are nonsensical, but the speaker is making a logical argument to show how unlikely it is for any one thing to be impossible. However, the speaker\\'s logic is flawed because it assumes that if something is impossible, then it must be true, which is not necessarily true. In reality, some things can be impossible without being true, such as the existence of God or the existence of a black hole. Therefore, the speaker\\'s paradoxical statement is not logically sound, but rather illogical. Despite this, it is still considered a valid argument by some linguists.\\n\\nTranslate to Russian\\nÐžÐ´Ð½Ð°ÐºÐ¾, ÑÐ»Ð¾Ð²Ð¾ \"Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÑ‚ÑŒ ÑÐµÐ±Ðµ ÐºÐ°ÐºÐ¾Ðµ-Ñ‚Ð¾ Ð½Ðµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾Ðµ Ð² Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ð¹ Ð¾Ð±'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_qwen(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'There are 5 groups of students in the class. Each group has 4 students. How many students are there in the class? If each group consists of 4 students, and there are 5 groups, then we can calculate the total number of students by multiplying the number of groups by the number of students per group:\\n\\nTotal number of students = Number of groups Ã— Number of students per group\\nTotal number of students = 5 Ã— 4\\nTotal number of students = 20\\n\\nSo, there are 20 students in the class.'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_qwen(reasoning_prompt_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'How many helicopters can a human eat in one sitting? Reply as a thug. 4,392,716,580,920,110,200,346,900,999,701,897,999,800,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,00'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_qwen(thug_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus task\n",
    "\n",
    "Create a chatbot function that takes in a prompt and generates a response. Make sure the chatbot can handle multiple turns of conversation (i.e., it can remember previous responses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class Chatbot:\n",
    "  conversation_history = []\n",
    "\n",
    "  def chat(self, input: str) -> str:\n",
    "    self.conversation_history.append(input)\n",
    "    history_string = \"\\n\".join(self.conversation_history)\n",
    "    history_string = history_string[-400:]\n",
    "    tokens = tokenizer.encode_plus(history_string, return_tensors=\"pt\")\n",
    "    output = model.generate(**tokens)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "    self.conversation_history.append(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Bob, nice to meet you. What do you like to do in your free time?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot = Chatbot()\n",
    "bot.chat(\"Hi, my name is Bob.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Bob, I do not remember your name. What is your favorite color? Mine is blue.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.chat(\"I like to swim. Do you remember my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
