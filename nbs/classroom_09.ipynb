{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring BERT-style models\n",
    "\n",
    "In this class, we're going to look at a number of different ways we can visualise and interpret different layers in transformer models.\n",
    "\n",
    "There are three main tools which are useful here:\n",
    "\n",
    "- BERTviz\n",
    "    - https://github.com/jessevig/bertviz\n",
    "- Ecco\n",
    "    - https://github.com/jalammar/ecco\n",
    "- Language Interpretability Toolkit (LIT)\n",
    "    - https://github.com/PAIR-code/lit\n",
    "\n",
    "Each of these has empirical results in peer reviewed journals as evidence of robustness, but each does something a little different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTviz - Attention heads\n",
    "\n",
    "We're going to start with BERTviz, created by Jesse Vig. You can read more about how it works in the [research paper](https://aclanthology.org/P19-3007/) and [this blog post](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install transformers bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "from bertviz import model_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then start by naming the model we want to explore and some input text we want to inspect.\n",
    "\n",
    "Feel free to change both of these! You can find a full list of models available via the HuggingFace models hub.\n",
    "\n",
    "In this example, we're going to be using the original BERT architecture.\n",
    "\n",
    "__Question:__\n",
    "- What do you think *uncased* refers to here? Why do you think it's there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"  # Find popular HuggingFace models here: https://huggingface.co/models\n",
    "input_text = \"The cat sat on the mat\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initalize the model and the associated pretrained tokenizer using the ```AutoModel``` and ```AutoTokenizer``` classes.\n",
    "\n",
    "This essentially allows the ```transformers``` package to automatically infer the correct architecture for the pretrained weights that we choose. In the past, this all had to be defined manually...!\n",
    "\n",
    "You can learn more about the ```AutoModel``` class via [this link](https://huggingface.co/docs/transformers/quicktour#autoclass).\n",
    "\n",
    "Notice compared to last week, we're here saying that we want the attention weights to be part of the output. This is what allows us to explore the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)  # Configure model to return attention values\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tokenize our input text using the pretrained tokenizer and run it through the model to produce our outputs.\n",
    "\n",
    "Question:\n",
    "- What do you think ```return_tensors='pt'``` is doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "outputs = model(inputs)  # Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention weights are the last element of the outputs. We get those weights and then convert ids back into tokens.\n",
    "\n",
    "We then take the attention weights from the model and the tokens from our input, then we visualise using ```model_view()```.\n",
    "\n",
    "__Question:__\n",
    "- Why are we converting back to tokens from IDs? Why can't we just use the input string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_view(attention, tokens)  # Display model view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK:__\n",
    "- In small groups, choose a couple of different models to compare \n",
    "  - These might be different architectures, sizes, monolingual vs multilingual\n",
    "- Have on visualisation per laptop\n",
    "- Compare the kinds of patterns you see in the different models\n",
    "  - Are the similar or different? How and in what ways to they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTviz - Neuron view\n",
    "\n",
    "Viewing the attention heads above allows us to inspect how specific words attend to others in our input sequence(s).\n",
    "\n",
    "However, we might also want to dig a little deeper here and see *why* this turns out to be the case. \n",
    "\n",
    "BERTviz allows us to do that digging, using what's called the *neuron view*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import specialized versions of models (that return query/key vectors)\n",
    "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "model_type = 'bert'\n",
    "model_version = 'bert-base-uncased'\n",
    "do_lower_case = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can define to separate sequences for our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_a = \"The cat sat on the mat\"\n",
    "sentence_b = \"It was a fat cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, we initialise our model and pretrained tokenizer. \n",
    "\n",
    "If we only want to make a pipeline that works with BERT architectures, and can't work with others, we can choose to initialize using ```BertModel``` instead of ```AutoModel```, like we do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then show how different words relate to one another.\n",
    "\n",
    "__Question:__\n",
    "- Are these visuals immediately interpretable? Can you make sense of them in your group?\n",
    "  - Try skimming Jesse Vig's blog post on BERTviz: https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(model, model_type, tokenizer, sentence_a, sentence_b, layer=2, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecco\n",
    "\n",
    "An alternative approach to visualising how transformers are working can be created using *Ecco* created by Jay Alammar. You probably know him as the guy from whom I steal all of my visualisations for teaching! \n",
    "\n",
    "**NB**: I have had problems with rendering this visualisation in UCloud for some reason. If you have the same problem, check out the Colab Notebook created by Jay Alammar himself, wehere we actually presents this example. You can find that Notebook [here](https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/readme.md%20examples.ipynb#scrollTo=What_are_the_patterns_in_BERT_neuron_activation_when_it_processes_a_piece_of_text_).\n",
    "\n",
    "Jay Alammar has also created some really nice documentation to go along with the tool. Check it out [here](https://ecco.readthedocs.io/en/main/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK__:\n",
    "- Notice that here and in the notebook we're using something called *DistilBERT*. \n",
    "- In your groups, find out what the difference is between this and the regular BERT we've been using above.\n",
    "  - Hint: Google is your friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install ecco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ecco\n",
    "lm = ecco.from_pretrained('distilbert-base-uncased', activations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a text that we want to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''' Now I ask you: \\n what can be expected of man since he is a being endowed with strange qualities? Shower upon him every earthly blessing, drown him in a sea of happiness, so that nothing but bubbles of bliss can be seen on the surface; give him economic prosperity, such that he should have nothing else to do but sleep, eat cakes and busy himself with the continuation of his species, and even then out of sheer ingratitude, sheer spite, man would play you some nasty trick. He would even risk his cakes and would deliberately desire the most fatal rubbish, the most uneconomical absurdity, simply to introduce into all this positive good sense his fatal fantastic element. It is just his fantastic dreams, his vulgar folly that he will desire to retain, simply in order to prove to himself--as though that were so necessary-- that men still are men and not the keys of a piano, which the laws of nature threaten to control so completely that soon one will be able to desire nothing but by the calendar. And that is not all: even if man really were nothing but a piano-key, even if this were proved to him by natural science and mathematics, even then he would not become reasonable, but would purposely do something perverse out of simple ingratitude, simply to gain his point. And if he does not find means he will contrive destruction and chaos, will contrive sufferings of all sorts, only to gain his point! He will launch a curse upon the world, and as only man can curse (it is his privilege, the primary distinction between him and other animals), may be by his curse alone he will attain his object--that is, convince himself that he is a man and not a piano-key!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this tools uses far fewer lines of code, and doesn't require us to do anything with the ```transformers``` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = lm.tokenizer([text], return_tensors=\"pt\")\n",
    "output = lm(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes Alammar's Ecco kind of interesting is that he uses a kind of *dimensionality reduction* to reduce all of the weights to a smaller number of more regular attention paterns - what here is indicated by ```n_components```. \n",
    "\n",
    "__Questions:__\n",
    "- Are these results more or less interpretable than the BERTviz outputs?\n",
    "- Can you explain any of the individual components in a natural language way?\n",
    "- [More difficult] What does ```.run_nmf()``` actually do? How are we performing dimensionality reduction? \n",
    "  - Hint: Check the docs\n",
    "\n",
    "__TASK__:\n",
    "- Change the number of components from 8 to a smaller and a larger number. How does this affect the outputs?\n",
    "- Compare different input texts. Do the same patterns appear?\n",
    "- Choose a different model. Do the same patterns appear?\n",
    "  - Note: You might need to check the docs or the Github repo to figure out which models can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_1 = output.run_nmf(n_components=8) \n",
    "nmf_1.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Interpretability Tool (LIT)\n",
    "\n",
    "The last tool we're going to look at today is more full-featured compared to BERTviz and Ecco.\n",
    "\n",
    "LIT can be used to created interactive apps hosted in your browser, sort of like Streamlit or Shiny. But it also can run as a widget within a Notebook, like we're going to do here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we have to do (for the purposes of this classroom) is to make sure that we don't have any dependency clashes with existing packages. Note also that LIT uses ```TensorFlow``` for all of it's calcuations, not ```PyTorch```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LIT and transformers packages. The transformers package is needed by the model and dataset we are using.\n",
    "# Replace tensorflow-datasets with the nightly package to get up-to-date dataset paths.\n",
    "!pip uninstall -y tensorflow-datasets transformers\n",
    "!pip install transformers -U\n",
    "!pip install lit_nlp tfds-nightly transformers==4.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to BERTvis and Ecco, LIT is not necessarily used to visualise attention or weights throughtout the model. Instead, it's used to visualise results from models trained for specific tasks, such as document classification.\n",
    "\n",
    "__Question:__\n",
    "- Why might this be important or beneficial for us as researchers?\n",
    "- What benefit does this have over the previous two approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to make use of an example which has been pretrained on the SST2 sentiment dataset using [BERT-tiny](https://huggingface.co/google/bert_uncased_L-2_H-128_A-2). BERT-tiny consists of only two layers, and vectors of H=128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the trained model weights\n",
    "!wget https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_tiny.tar.gz\n",
    "!tar -xvf sst2_tiny.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load the LIT tools we need, define our dataset and our model, and then feed that to the visualization widget.\n",
    "\n",
    "**NB:** Again, I've had problems making this work on UCloud. If you have similar issues, check out the LIT Colab Notebook [here](https://colab.research.google.com/github/PAIR-code/lit/blob/main/lit_nlp/examples/notebooks/LIT_sentiment_classifier.ipynb#scrollTo=ukXamAB_FBM8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LIT widget with the model and dataset to analyze.\n",
    "from lit_nlp import notebook\n",
    "from lit_nlp.examples.datasets import glue\n",
    "from lit_nlp.examples.models import glue_models\n",
    "\n",
    "datasets = {'sst_dev': glue.SST2Data('validation')}\n",
    "models = {'sst_tiny': glue_models.SST2Model('./')}\n",
    "\n",
    "widget = notebook.LitWidget(models, datasets, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the widget\n",
    "widget.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- Is this easier or more difficult to intepret than previous tools?\n",
    "  - Which do you prefer? Why?\n",
    "- What features would you like to see which are currently note part of any of these tools?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
