{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "\n",
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/ucloud/.local/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/ucloud/.local/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/ucloud/.local/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ucloud/.local/lib/python3.12/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in /home/ucloud/.local/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/ucloud/.local/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ucloud/.local/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ucloud/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ucloud/.local/lib/python3.12/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ucloud/.local/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/ucloud/.local/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ucloud/.local/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/ucloud/.local/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ucloud/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/ucloud/.local/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ucloud/.local/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ucloud/.local/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ucloud/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ucloud/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ucloud/.local/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucloud/.local/lib/python3.12/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucloud/.local/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucloud/.local/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.25.0-py3-none-any.whl (436 kB)\n",
      "Downloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, tqdm, safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.9.0 huggingface-hub-0.25.0 regex-2024.9.11 safetensors-0.4.5 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.44.2 typing-extensions-4.12.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /home/ucloud/.local/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ucloud/.local/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /home/ucloud/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ucloud/.local/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from torch) (68.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch)\n",
      "  Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ucloud/.local/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl (797.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 sympy-1.13.2 torch-2.4.1 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we talked about how ```Hugging Face``` is the place to go for pretrained models. Today, we're going to meet ```gensim``` which is a library for working with (static) word embeddings like word2vec. You can find the documentation [here](https://radimrehurek.com/gensim/). Additionally, ```scikit-learn``` provides a whole host of fundamental machine learning algortithms\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[```GloVe```](https://nlp.stanford.edu/projects/glove/) word vectors are trained on aggregated global word-word co-occurrence statistics from a corpus. glove-wiki-gigaword-300 are 300-dimensional vectors trained on co-occurence statistics from Wikipedia articles and various news outlets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "embeddings = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've outlined a couple of tasks for you below to experiment with. Use these as a starting point to explore the nature of word embeddings and how they work.\n",
    "\n",
    "Work in small groups on these tasks and make sure to discuss the issues and compare results.\n",
    "\n",
    "## Task 1 - Synonyms & antonyms\n",
    "\n",
    "As introduced in the lecture, we can use _cosine similarity_ to quantify the similarity between words embeddings, since words with similar meanings tend to have vectors pointing in similar directions, resulting in a higher cosine similarity. We can leverage this relationship for various tasks in NLP, such as finding synonyms and antonyms.\n",
    "\n",
    "_Cosine similarity_ can also be thought of as _cosine distance_, which is simply ```1 - cosine similarity```. So the higher the cosine distance, the further away two words are from each other and so they have less \"in common\". Consequently, we would expect synonyms to have a lower cosine distance than antonyms.\n",
    "\n",
    "You can use the the ```embeddings.distance()``` function to compute the cosine distance between two words and the ```embeddings.most_similar()``` function to examine what other words are closest to a target word. Here is a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('physician', 0.704085111618042),\n",
       " ('doctors', 0.6507135033607483),\n",
       " ('medical', 0.5991251468658447),\n",
       " ('dr.', 0.5985949039459229),\n",
       " ('surgeon', 0.5897458791732788),\n",
       " ('nurse', 0.5859817266464233),\n",
       " ('hospital', 0.568318784236908),\n",
       " ('dentist', 0.5561408996582031),\n",
       " ('patient', 0.5560954809188843),\n",
       " ('pharmacist', 0.533785879611969)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(\"doctor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29591500759124756"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.distance(\"doctor\", \"physician\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find three words ```(w1,w2,w3)``` where ```w1``` and ```w2``` are synonyms and ```w1``` and ```w3``` are antonyms, but where:\n",
    "\n",
    "```Cosine Distance(w1,w3) < Cosine Distance(w1,w2)```\n",
    "\n",
    "For example, w1=\\\"happy\\\" is closer to w3=\\\"sad\\\" than to w2=\\\"cheerful\\\".\n",
    "\n",
    "I've given a starting example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43471425771713257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.distance(\"happy\", \"sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5596834421157837"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.distance(\"happy\", \"cheerful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.distance(\"happy\", \"sad\") < embeddings.distance(\"happy\", \"cheerful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('really', 0.6171224117279053),\n",
       " (\"'m\", 0.616436243057251),\n",
       " ('better', 0.612736165523529),\n",
       " ('sure', 0.6120087504386902),\n",
       " ('very', 0.6063336730003357),\n",
       " ('i', 0.6053487062454224),\n",
       " (\"'ll\", 0.6028794050216675),\n",
       " ('pleased', 0.6019134521484375),\n",
       " ('pretty', 0.5985657572746277),\n",
       " ('get', 0.5881876945495605)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive = [\"good\", \"happy\"],\n",
    "                        negative=[\"evil\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have found your example,\n",
    "- __give a possible explanation for why this counter-intuitive result may have happened.__\n",
    "\n",
    "## Task 2 - Analogies\n",
    "\n",
    "We saw in the lecture that we can use basic arithmetic on word embeddings, in order to conduct a word analogy task.\n",
    "\n",
    "For example:\n",
    "\n",
    "```king - man + woman = queen```\n",
    "\n",
    "If we take the vector for ```king``` and subtract the vector for ```man```, we're removing the gender component from the ```king```. If we then add ```woman``` to the resulting vector, we should be left with a vector similar to ```queen```.\n",
    "\n",
    "NB: It might not be _exactly_ the vector for ```queen```, but it should at least be _close_ to it.\n",
    "\n",
    "```gensim``` has some quirky syntax that allows us to perform this kind of arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.6713277101516724)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=['king', 'woman'], \n",
    "                   negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to\n",
    "- __find at least three analogies which correctly hold - where \"correctly\" means that the closest vector corresponds to the word that you expect it would.__\n",
    "\n",
    "## Task 3 - Wrong analogies\n",
    "\n",
    "Can you \n",
    "- __find any analogies which should hold but don't?__\n",
    "- __Why don't they work? Are there any similarities or trends?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnegative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/gensim/models/keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[0;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key '' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "embeddings.most_similar(positive=['', ''], \n",
    "                   negative=[''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Bias\n",
    "\n",
    "As we spoke briefly about in the lecture, word embeddings tend to display bias of the kind found in the training data.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('doctor', 0.4778822362422943)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=['nurse', 'man'], \n",
    "                   negative=['woman'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some of the techniques you've worked on above, \n",
    "- __can you find some clear instances of bias in the word embeddings?__\n",
    "\n",
    "## Task 5 - LLMs\n",
    "\n",
    "Try to load in the language model from last week and figure out how best to prompt it to generate analogies.\n",
    "\n",
    "- __How does it compare to the word embeddings?__ \n",
    "- __What are the advantages and disadvantages of each approach?__ \n",
    "- __Does it seem to be biased in the same way as the word embeddings?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0339b0744374625b06e1a5a6a0efcf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029e649d7c7f424f88288fb14da0ba87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e799115a2cf94ef4826ccc138e227abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04c6e0311604015ac12f710285c2a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924a4257f4334194a8df1c733e592963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d471cc9c040448ba36819664a9b9d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a147e29ef7804b29839b047ce66153fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers \n",
    "import torch\n",
    "\n",
    "model = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 - Visualisation\n",
    "\n",
    "In the following cell, I've written a short bit of code which takes a given subset of words and plots them on a simple scatter plot. Remember that the word embeddings are 300D, so we need to perform some kind of dimensionality reduction on the embeddings to get them down to 2D.\n",
    "\n",
    "Here, I'm using a simple PCA algorithm implemented via ```scikit-learn```. An alternative approach might also be to use Singular Value Decomposition or SVD, which works in a similar but ever-so-slightly different way to PCA. You can read more [here](https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/) and [here](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491).\n",
    "\n",
    "Experiment with plotting certain subsets of words by changing the ```words``` list.\n",
    "\n",
    "- __How useful do you find these plots?__\n",
    "- __Do they show anything meaningful?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the list of words we want to plot\n",
    "words = [\"man\", \"woman\", \"doctor\", \"nurse\", \"king\", \"queen\", \"boy\", \"girl\"]\n",
    "\n",
    "# an empty list for vectors\n",
    "X = []\n",
    "# get vectors for subset of words\n",
    "for word in words:\n",
    "    X.append(embeddings[word])\n",
    "\n",
    "# Use PCA for dimensionality reduction to 2D\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# or try SVD - how are they different?\n",
    "# svd = TruncatedSVD(n_components=2)\n",
    "# fit_transform the initialized PCA model\n",
    "# result = svd.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "# for each word in the list of words\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus tasks\n",
    "\n",
    "If you run out of things to explore with these embeddings, try some of the following tasks:\n",
    "\n",
    "- __make new plots like those above but cleaner and more informative__\n",
    "- __write a script which takes a list of words and produces the output above__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
